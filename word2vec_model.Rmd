---
header-includes:
  - \usepackage[L7x]{fontenc}
  - \usepackage{bbm}
  - \bibliographystyle{plainnat}
output:
 pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: no
    latex_engine: pdflatex
margin-left: 6.5in
margin-top: 9in
fontsize: 12pt    
papersize: A4
fontfamily: palatino
---
\vskip 20pt
\centerline{\bf \large VILNIUS UNIVERSITY}
\bigskip
\centerline{\large \textbf{FACULTY OF MATHEMATICS AND INFORMATICS}}
\vskip 120pt
\centerline{\bf \Large \textbf{Article}}
\vskip 50pt
\begin{center}
{\bf \LARGE Ridge and Lasso logistic regression}

\vspace{4mm}

\end{center}
\vskip 120pt
\centerline{\Large Eligijus Bujokas}
\vskip 120pt
\centerline{\large \textbf{VILNIUS (2018.12.21)}}

\newpage

\begin{center}{\large\textbf{Logistic Ridge and Lasso regression}}\end{center}

\vspace{2\baselineskip}
\begin{center}\textbf{Abstract}\end{center}

The aim of this paper is to implement Ridge and Lasso logistic regression to a textual dataset. 

The calculations and visuals are made using the statistical software $\textbf{R}$.

\vspace{\baselineskip}
\noindent\textbf{Key words :}
Logistic regression, Ridge, Lasso

\vspace{\baselineskip}

\newpage

\tableofcontents

\newpage

```{r setup, include = FALSE}
library(data.table)
library(dplyr)
library(knitr)
library(tm)
library(textmineR)
library(text2vec)
library(RefManageR)
bib <- ReadBib("bibliography.bib", check = FALSE)
```

# Introduction

In the modern world there are many clasification tasks that revolve around clasifying textual data. There are algorythms that based on a user inputed string can talk with a person, after reading lyrics of a song it can label the genre of the song and so on. Typical features (independant variables, regresors) are unique words thus even a small text can have thousands of collumns in the design matrix. 

A big design matrix leads to big computational times and often leads to multicolinearity. The reduction of the number of features in a dataset or reweighting the importance of features often leads to a speed up in the computational time and better results. Ridge and Lasso regression are often the tools of choice when dealing with the mentioned problems.

\newpage

# Logistic regression for textual data

## Introductory case

Let us assume that our $\mathbb{Y}$ variable is binary. Each class of $\mathbb{Y}$ could indicate whether a string is 'positive' or 'negative' in a semantic sense, 'sincere' or 'not sincere' and so on. Each column in the $\mathbb{X}$ matrix are also binary. Each collumn represent a unique word and each row value represent whether a word was observed in a given string or not.

For example, let us say we have a 'positive' review -'This was delicious'- and a 'negative' one -'The food was awfull'. Then raw document will have two rows:

```{r, echo = FALSE, results = 'asis'}
sentiment <- c(1, 0)
review <- c('This was delicious', 'This was awfull')
d <- data.table(sentiment, review)
kable(d)
```

The matrix that is used for computations is often called the document term matrix (dtm for short). The dtm of our raw document would look like this: 

```{r, echo = FALSE, results = 'asis'}
prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(d$review, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = 1:nrow(d), 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm = create_dtm(it_train, vectorizer) %>% 
  as.matrix() %>% 
  data.table()
kable(dtm)
```

As we can see, the matrix indicates that the first review had words $\textit{delicious}$, $\textit{was}$ and $\textit{this}$ while the second review had the words $\textit{awfull}$, $\textit{was}$ and $\textit{this}$. 

We can define a general linear model for this case:

$$ log \dfrac{P(Y = 1)}{P(Y = 0)} = \beta_{0} + \beta_{1} \mathbbm{1}_{delicious} + \beta_{2} \mathbbm{1}_{was} + \beta_{3} \mathbbm{1}_{this} + \beta_{4} \mathbbm{1}_{awfull}$$

According to the data, the coefficient $\beta_{1}$ should be positive and $\beta_{4}$ should be negative implying positive and negative sentiments respectivelly. The words $\textit{was}$ and $\textit{this}$ do not help in distinguishing between the positive and the negative sentiments thus the coefficients near these features need to be 0 or very small. 

\newpage

## General case for binary dependant variable

Let us assume that our data is the set: 

$$D_{\mathbb{Y}, \mathbb{X}} = \{ \mathbb{Y}_{i} \in \{0, 1\}, \mathbb{X} = [1,X_{i1}, X_{i2}, ..., X_{ik}] , \forall i \in \{1, ..., n\}\}$$

In most practical cases, the intercept is added to the design matrix.

In matrix form: 

$$\mathbb{Y} = \begin{bmatrix}
    y_{1}\\
    y_{2}\\
    \vdots \\
    y_{n}
\end{bmatrix} 
\mathbb{X} = \begin{bmatrix}
    1 & x_{11} & x_{12} & ... & x_{1k}\\
    1 & x_{21} & x_{22} & ... & x_{2k}\\
    ... & ... & ... & ...\\
    1 & x_{n1} & x_{n2} & ... & x_{nk}
\end{bmatrix}$$

In practise, all $x_{ij}$ are either 0 or 1, indicating existance in certain row of the document term matrix, or the count of occurance in row in the document term matrix. The number of columns in the $\mathbb{X}$ matrix is equal to the number of unique words in our document.

The general model is then:

$$ log \dfrac{P(Y = 1)}{P(Y = 0)} = \sum_{i = 0}^{k}\beta_{i} X_{i} \, (1)$$

\newpage

## Maximum likelihood for estimating the coefficients

Let us define: 

$$\theta := \begin{bmatrix}
    \beta_{0}\\
    \beta_{1}\\
    \vdots \\
    \beta_{k}
\end{bmatrix}$$

From (1), the probability of 'success' for each observation i can be rewritten as `r Cite(bib, 'ML', .opts = list(cite.style = "numeric"), after=" , pp. 3-5")`:

$$ \pi_{i} := P(\mathbb{Y}_{i} = 1) = \dfrac{e^{\theta^{T} \mathbb{X}_{i}}}{1 + e^{\theta^{T} \mathbb{X}_{i}}} = \dfrac{1}{1 + e^{-\theta^{T} \mathbb{X}_{i}}} $$

$$ P(\mathbb{Y}_{i} = 0) = 1 - \pi_{i} $$

In every binary case, the $\mathbb{Y}$ can be encoded as a vector consisting of 0 and 1. Thus, we want $\theta$ such that the product:

$$ l(\theta) = \prod_{i = 1}^{n} \pi_{i}^{y_{i}}(1 - \pi_{i})^{1 - y_{i}} \, (2)$$

is the biggest . 

Logarythm is a monotone function thus the maximum of $l(\theta)$ is the same as $log(l(\theta))$ `r Cite(bib, 'ridge_lasso', .opts = list(cite.style = "numeric"), after=" , pp. 636 - 637")`. 

$$L(\theta) := log(l(\theta)) = \sum_{i=1}^{n}\left[y_{i} log(\pi_{i}) + (1 - y_{i})log(1 - \pi_{i})\right] = \sum_{i=1}^{n}\left[y_{i}log(\dfrac{\pi_{i}}{1 -\pi_{i}}) + log(1 - \pi_{i})\right] $$

$$L(\theta) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] \, (3)$$

The $\widehat{\theta}$ that maximizes the (3) equation will give a weight to every unique word in our text. This, in practise, is ussualy not ideal, because even a small text document can have thousands of unique words. 

\newpage

## Ridge logistic regression 

Ridgre logistic regression introduces an additional term to the (3) equation - the L2 penalty.

$$ L^{R}(\theta, \lambda) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] - \lambda \sum_{j = 1}^{k} \beta_{j}^{2} $$

Often in practise, the $\lambda$ parameter is fixed to a certain value. As the parameter $\lambda$ increases, the ridge coefficient estimates will tend to approach zero. However, the penalty introduced in the log-likelihood function will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero. Hence, ridge regression has the disadvantage over model selection, of including all the predictors in the final model. 

On the other hand, Ridge regression estimates gives us more uniformally distributed weights to all words. Depending on the problem, one may view this as an advantage. 

## Lasso logistic regression 

Lasso logistic regression is very similar to that of Ridge, but the penalty term is L1:

$$ L^{L}(\theta, \lambda) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] - \lambda \sum_{j = 1}^{k} |\beta_{j}| $$

The L1 penalty used in the lasso is used for both variable selection and shrinkage, since it has the effect, when the $\lambda$ is sufficiently large, of forcing some of the coefficient estimates to be exactly equal to zero `r Cite(bib, 'ridge_lasso', .opts = list(cite.style = "numeric"), after=" , pp.637")`.
In Lasso regression, the final model may involve only a subset of the predictors, which in turn improves model interpretability. 

Depending on the research subject and the problem, having less predictors is beneficial. 

## Elastic net regression

One is not confined to just using either Ridge or Lasso regression. The elastic net procedure tries to implement both of these methods: 

$$ L^{EN}(\theta, \lambda) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] -( \lambda \sum_{j = 1}^{k} |\beta_{j}| + (1 - \lambda)\sum_{j = 1}^{k} \beta_{j}^{2})$$

$\lambda \in (0, 1)$.  

This approach is particularly useful when the number of predictors is much larger than the number of
observations `r Cite(bib, 'EN', .opts = list(cite.style = "numeric"))`. 

\newpage

# Quora question  example


\newpage

# References

```{r references, results="asis", echo=F, warning=F}
PrintBibliography(bib)
```
