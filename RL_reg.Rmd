---
header-includes:
  - \usepackage[L7x]{fontenc}
  - \usepackage{bbm}
  - \bibliographystyle{plainnat}
output:
 pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: no
    latex_engine: pdflatex
margin-left: 6.5in
margin-top: 9in
fontsize: 12pt    
papersize: A4
fontfamily: palatino
---

```{r setup, include = FALSE}
library(data.table)
library(dplyr)
library(knitr)
library(tm)
library(textmineR)
library(text2vec)
library(RefManageR)
library(stopwords)
library(stringr)
library(glmnet)
library(pluralize)
library(qdap)
bib <- ReadBib("bibliography.bib", check = FALSE)
knitr::opts_chunk$set(comment = NA)
```

\vskip 20pt
\centerline{\bf \large VILNIUS UNIVERSITY}
\bigskip
\centerline{\large \textbf{FACULTY OF MATHEMATICS AND INFORMATICS}}
\vskip 120pt
\centerline{\bf \Large \textbf{Article}}
\vskip 50pt
\begin{center}
{\bf \LARGE Ridge and Lasso logistic regression in natural language procesing}

\vspace{4mm}

\end{center}
\vskip 120pt
\centerline{\Large Eligijus Bujokas}
\vskip 120pt
\centerline{\large \textbf{VILNIUS (2018.12.18)}}

\newpage


\begin{center}{\large\textbf{Logistic Ridge and Lasso regression in NLP}}\end{center}

\vspace{2\baselineskip}
\begin{center}\textbf{Abstract}\end{center}

The aim of this document is to give a brief introduction to natural language procesing (NLP), apply Ridge, Lasso and elastic net binary logistic regression to an NLP related problem and to write a reproducible code.   

The calculations and visuals are made using the statistical software $\textbf{R}$. The main framework for text preprocesing is the $\textbf{text2vec}$ `r Cite(bib, 'text2vec')` package. 

The logistic regression is implemented using the package $\textbf{glmnet}$ `r Cite(bib, 'glmnet')`.

\vspace{\baselineskip}
\noindent\textbf{Key words :}
Logistic regression, Ridge, Lasso, Elastic Net, NLP, machine learning, tfidf

\vspace{\baselineskip}


\newpage

\tableofcontents

\newpage

# Introduction

There are many classification tasks that revolve around classifying textual data. There are algorithms that based on a user inputed string can talk with a person, after reading lyrics of a song it can label the genre of the song and so on. The field of computer science that deals with text which comes from users (humans) is called natural language processing. 

Natural language processing (NLP) is a subfield of computer science concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

One part of NLP tasks is to deal with binary responses based on text. A computer must determine whether an inputed text has a positive or negative sentiment, whether this is hate speech or not and so on. It is very time consuming (and almost impossible) for humans to cover the vast amount of textual data and give it a label thus computer scientists implemented binary logistic regression in various NLP frameworks to deal with the labeling problem. Instead of goind through millions of documents, a group of people need to label only a couple of thousand of documents and have the computer (machine) fit the binary logistic model (learn) and fit the model to other texts.

\newpage

## Introductory example

Let us assume that our $\mathbb{Y}$ variable is binary. Each class of $\mathbb{Y}$ could indicate whether a string is 'positive' or 'negative' in a semantic sense, 'sincere' or 'not sincere' and so on. Each column in the $\mathbb{X}$ matrix are also binary. Each column represent a unique word and each row value represent whether a word was observed in a given string or not.

For example, let us say we have a couple of strings regarding a car review and some regarding other news:

```{r, echo = FALSE, results = 'asis'}
d <- data.table(text = c('the new car', 
                  'opel is good automobile', 
                  'the weather is dreadful', 
                  'the stocks are rising'), 
                is_car_review = c(1, 1, 0, 0))
kable(d)
```

The matrix that is used for computations is often called the document term matrix (dtm for short). The dtm of our raw document would look like this: 

```{r, echo = FALSE, results = 'asis'}
prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(d$text, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = 1:nrow(d), 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm = create_dtm(it_train, vectorizer) %>% 
  as.matrix() %>% 
  data.table()
kable(dtm)
```

Intuitively, we would like the words 'car' and 'automobile' to indicate a review beeing from a car review and the words 'weather', 'stocks' and others to indicate a non car related review. 

Binary logistic regression is a method that can assign each feature (word in our case) a positive or a negative weight depending on the response variable (variable $\textbf{is car review}$ in our case).

\newpage

# NLP definitions and techniques

## Document corpus

A text corpus is a collection of texts of written (or spoken) language presented in electronic form. It is ussually denoted as 
$\mathbb{D}$. Typically, the corpus is the vector containing the text data. In the introduction chapter, the text corpus would be ['the new car', 'opel is good automobile',  'the weather is dreadful', 'the stocks are rising']. 

Each document are made of several tokens. 

## Tokenization 

Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numeric, etc. This process is called tokenization `r Cite(bib, 'token', .opts = list(cite.style = "numeric"))`.

In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both 'Los Angeles' and 'rock 'n' roll' are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like 'I'm' into separate words 'I' and 'am'.

Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation.

For example, the string 'I'm a from the city of Vilnius' can be tokenized into several tokens: 'i', 'am', 'from', 'the', 'city', 'of', 'vilnius'. 

Each term (token) in a given corpus is denoted as $t$. 

## N - grams

An n-gram is a contiguous sequence of n items from a given sequence of text `r Cite(bib, 'nlp_bible', .opts = list(cite.style = "numeric"), after=" , pp. 37")`. Given a sentence, we can construct a list of n-grams from the sentence by finding pairs of words that occur next to each other. For example, given the sentence 'my name is Eligijus' you can construct bigrams (n-grams of length 2) by finding consecutive pairs of words: 'my name', 'name is' and 'is Eligijus'. Each of these bigrams would be considered a token and would be considered as a separate feature in the $\mathbb{X}$ matrix.

## Tf-idf tranformation

Tf-idf, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf-idf is the product of two statistics, term frequency and inverse document frequency. 

### Term frequency

$tf(\centerdot)$ is a function of two arguments - the term t and document d. If we denote the term frequency in a document as $f_{t, d}$ then 

$$ tf(t, d) = f_{t, d} $$

In simpler terms, the function tf() takes every unique token in our corpus and counts how many times it appeared in a document.

### Inverse document frequency

The inverse document frequency ($idf(\centerdot)$) is a measure of how much information the word provides, in other words, if it is common or rare across all documents.

$$ idf(t, D) = log\left(\dfrac{N}{1 + |\{d \in D: t \in d  \}|} \right)$$  

N - total number of documents in our corpus. The corpus in the introduction chapter has two documents. 

$|\{d \in D: t \in d  \}|$  number of documents where the term t appears. We add 1 because if no document has a certain term, then we would be dividing by zero. 

\newpage

### Tf - idf calculation 

The tf-idf transformation for each term is a product of term frequency and inverse document frequency. 

$$tfidf(t, d, D) = tf(t, d) \, idf(t, D)$$

Recall our example case. Lets us calculate the tfidf statistic for the word 'car'. 

Term frequency:

$tf('car', d_{1}) = 1$

$tf('car', d_{2}) = 0$

$tf('car', d_{3}) = 0$

$tf('car', d_{4}) = 0$

Inverse document frequency:

$idf('car', D) = log(\frac{4}{2}) = 0.69$ 

Term frequency - inverse document frequency:

$tfidf('car', d_{1}, D) = 0.69$

$tfidf('car', d_{2}, D) = 0$

$tfidf('car', d_{3}, D) = 0$

$tfidf('car', d_{4}, D) = 0$

The transformation would yield that in the first document the term 'car' is equal to 0.69 and 0 in others. After transforming our dtm using this transformation we would diminish the weights of words that appear in every document and enhance the weights of specific words that are only in few documents. 

Additionally, we would not have to artificially drop words that are very common because the more times a term appears in documents, the less important it will be, because the dtm value for that feature will be close to 0.

\newpage

# Logistic regression for textual data

## General case for binary dependant variable

Let us assume that our data is the set: 

$$D_{\mathbb{Y}, \mathbb{X}} = \{ \mathbb{Y}_{i} \in \{0, 1\}, \mathbb{X} = [1,X_{i1}, X_{i2}, ..., X_{ik}] , \forall i \in \{1, ..., n\}\}$$

In most practical cases, the intercept is added to the design matrix.

In matrix form: 

$$\mathbb{Y} = \begin{bmatrix}
    y_{1}\\
    y_{2}\\
    \vdots \\
    y_{n}
\end{bmatrix} 
\mathbb{X} = \begin{bmatrix}
    1 & x_{11} & x_{12} & ... & x_{1k}\\
    1 & x_{21} & x_{22} & ... & x_{2k}\\
    ... & ... & ... & ...\\
    1 & x_{n1} & x_{n2} & ... & x_{nk}
\end{bmatrix}$$

In practice, all $x_{ij}$ are either 0 or 1, indicating that a feature appeared in a certain document of the corpus, or the count of occurrence in a given document. The number of columns in the $\mathbb{X}$ matrix is equal to the number of unique terms in our document.

The terms could be unigrams, bigrams, stemmed words, whole sentences. It all depends on the problem and the insights of the researcher.

The general model for the binary response variable is:

$$ log \dfrac{P(Y = 1)}{P(Y = 0)} = \sum_{i = 0}^{k}\theta_{i} X_{i} \, (1)$$

\newpage

## Maximum likelihood for estimating the coefficients

Let us define: 

$$\theta := \begin{bmatrix}
    \theta_{0}\\
    \theta_{1}\\
    \vdots \\
    \theta_{k}
\end{bmatrix}$$

From (1), the probability of 'success' for each observation i can be rewritten as `r Cite(bib, 'ML', .opts = list(cite.style = "numeric"), after=" , pp. 3-5")`:

$$ \pi_{i} := P(\mathbb{Y}_{i} = 1) = \dfrac{e^{\theta^{T} \mathbb{X}_{i}}}{1 + e^{\theta^{T} \mathbb{X}_{i}}} = \dfrac{1}{1 + e^{-\theta^{T} \mathbb{X}_{i}}} $$

$$ P(\mathbb{Y}_{i} = 0) = 1 - \pi_{i} $$

In every binary case, the $\mathbb{Y}$ can be encoded as a vector consisting of 0 and 1. Thus, we want $\theta$ that maximizes the product:

$$ l(\theta) = \prod_{i = 1}^{n} \pi_{i}^{y_{i}}(1 - \pi_{i})^{1 - y_{i}} \, (2)$$

Logarithm is a monotone function thus the maximum of $l(\theta)$ is the same as $log(l(\theta))$ `r Cite(bib, 'ridge_lasso', .opts = list(cite.style = "numeric"), after=" , pp. 636 - 637")`. 

$$L(\theta) := log(l(\theta)) = \sum_{i=1}^{n}\left[y_{i} log(\pi_{i}) + (1 - y_{i})log(1 - \pi_{i})\right] = \sum_{i=1}^{n}\left[y_{i}log(\dfrac{\pi_{i}}{1 -\pi_{i}}) + log(1 - \pi_{i})\right] $$

$$L(\theta) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] \, (3)$$

The $\widehat{\theta}$ that maximizes the (3) equation will give a weight to every unique word in our text. This, in practice, may lead to some computational problems, because even a small text document can have thousands of unique words. 

\newpage

## Ridge logistic regression 

Ridge logistic regression introduces an additional term to the (3) equation - the L2 penalty.

$$ L^{R}(\theta, \alpha) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] - \alpha \sum_{j = 1}^{k} \theta_{j}^{2} $$

Often in practice, the $\alpha$ parameter is fixed to a certain value. As the parameter $\alpha$ increases, the ridge coefficient estimates will tend to approach zero. However, the penalty introduced in the log-likelihood function will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero. Hence, ridge regression has the disadvantage over model selection, of including all the predictors in the final model. 

On the other hand, Ridge regression estimates gives us more uniformally distributed weights to all words. Depending on the problem, one may view this as an advantage. 

## Lasso logistic regression 

Lasso logistic regression is very similar to that of Ridge, but the penalty term is L1:

$$ L^{L}(\theta, \alpha) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] - \alpha \sum_{j = 1}^{k} |\theta_{j}| $$

The L1 penalty used in the lasso is used for both variable selection and shrinkage, since it has the effect, when the $\alpha$ is sufficiently large, of forcing some of the coefficient estimates to be exactly equal to zero `r Cite(bib, 'ridge_lasso', .opts = list(cite.style = "numeric"), after=" , pp.637")`.
In Lasso regression, the final model may involve only a subset of the predictors, which in turn improves model interpretability. 

Depending on the research subject and the problem, having less predictors is beneficial. 

## Elastic net regression

One is not confined to just using either Ridge or Lasso regression. The elastic net procedure tries to implement both of these methods: 

$$ L^{EN}(\theta, \alpha) = \sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] -( \alpha \sum_{j = 1}^{k} |\theta_{j}| + (1 - \alpha)\sum_{j = 1}^{k} \theta_{j}^{2})$$

$\alpha \in (0, 1)$.  

This approach is particularly useful when the number of predictors is much larger than the number of
observations `r Cite(bib, 'EN', .opts = list(cite.style = "numeric"))`. 

## Formula in glmnet

The general notion of optimizing a function in the computer science world is to minimize instead of maximize the objective function. 
This is the case in glmnet package. The general formula which yealds the $\widehat{\theta}$ is the following: 

$$-\dfrac{1}{N}\sum_{i}^{n}\left[y_{i}\theta^{T}\mathbb{X}_{i} + log(1 + e^{\theta^{T}\mathbb{X}_{i}})\right] + \lambda \left( \alpha \sum_{j = 1}^{k} |\theta_{j}| + (1 - \alpha)\sum_{j = 1}^{k} \theta_{j}^{2}\right) $$

If $\alpha$ = 0 then we have Ridge regression. If $\alpha$ = 1 then we have Lasso regression. $\lambda$ is a parameter that tell us how much weight should be put on the penalty of the parameters.

\newpage

# Quora sincere question  example

We will use the various types of logistic regression and text preprocesing techniques to a data set from the website Quora. On that website users can ask any question and other users can answer those question for recognition or special points. While most of the questions are sincere, some of the questions are dubious and it is not easy to keep track of all the questions for a human. Thus, machine learning algorithms can be used to determine whether an inputed question is genuine or not. 

## Explanatory data analysis

```{r, echo = FALSE, results = 'asis'}
D <- fread('data/quora.csv', showProgress = FALSE) %>% 
  select(question_text, target)
head(D[target == 0]) %>% kable()
```

```{r, echo = FALSE, results = 'asis'}
head(D[target == 1][1:2]) %>% kable()
```

```{r}
dim(D)
```

There are more than one million documents in the corpus. 

The target column obtains two values: 0 if the question is sincere and 1 if the question is not sincere. 

```{r, results = 'asis'}
D$target %>% 
  table() %>% 
  data.table() %>% 
  kable()
```

Only about 6 percent of the documents in the corpus have a negative sentiment. We will balance the data by taking the same number of sincere sentiment rows as the insincere ones.

```{r}
set.seed(1)
no_negative <- D[target == 1] %>% 
  nrow()
D <- D[,.SD[sample(1:.N, no_negative, replace = FALSE)] , by = .(target)]
```

## Training and test set split 

We will hold out some data when creating a model in order to see if our model will generalize well. Ideally, the proportion of two classes in the test data set must be similar to the overall data set.

We will hold out 30 percent of the original data set.

```{r}
prop <- 0.3
rows_in_test <- round(nrow(D) * prop, 0) 

## Sampling the row indices that will be in the test set
test_index <- sample(1:nrow(D), rows_in_test, replace = FALSE)

## Test set
test <- D[test_index]

## Train set 
train <- D[setdiff(1:nrow(D), test_index)]
```

\newpage

## Text preprocesing 

We will drop the punctuations, make everything lowercase and remove the stop words (this, the, a, etc).

```{r}
train <- train[, index := 1:.N]
prep_fun = function(x){
  x <- tolower(x)
  x <- str_replace_all(x, '[[:punct:]]', ' ')
  x <- removeNumbers(x)
  x <- removePunctuation(x)
  return(x)
}

it_train = itoken(train$question_text, 
             preprocessor = prep_fun, 
             tokenizer = word_tokenizer, 
             ids = train$index, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train, 
                          stopwords = stopwords(source = 'smart'))
```

We will prune the vocabulary to include only words that appear in more than 10 documents and do not appear more than in 50 percent of the documents. Finally, we will transform our document term matrix using the tf idf transformation.

```{r}
pruned_vocab = prune_vocabulary(vocab, doc_count_min = 10, 
                                 doc_proportion_max = 0.5)
vectorizer = vocab_vectorizer(pruned_vocab)

dtm_train  = create_dtm(it_train, vectorizer, type = 'dgCMatrix')

tfidf = TfIdf$new()
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
```

\newpage

## Number of features

We will create several models with different alpha values using the glmnet package `r Cite(bib, 'glm', .opts = list(cite.style = "numeric"))`. The first model ($\alpha$ = 0) will represent the Ridge logistic regression while the last one ($\alpha$ = 1) will represent the Lasso logistic regression.

```{r}
## Defining the range of alpha

alpha_range <- seq(0, 1, length.out = 20) %>% 
  round(3)

## Constructing a list to store the values and the models in 

result_coef <- vector('list', length(alpha_range))
names(result_coef) <- alpha_range

result_model <- vector('list', length(alpha_range))
names(result_model) <- alpha_range

## Iterating over all alphas

for(alpha in alpha_range){
  model <- glmnet(x = dtm_train, y = train$target,
                  family = 'binomial',
                  alpha = alpha,
                  lambda = 0.01)
  coef_table <- data.table(feature = rownames(coef(model)), 
                           coefficient = coef(model)[, 1]) %>% 
    .[order(-coefficient)] %>% 
    .[coefficient!=0]
  result_coef[[as.character(alpha)]] <- coef_table %>% 
    .[, alpha := alpha]
  
   result_model[[as.character(alpha)]] <- model
}

```

\newpage

```{r, results = 'asis'}
no_ft <- lapply(result_coef, nrow) %>% 
  unlist()
result_frame <- data.table(alpha = alpha_range, no_features = no_ft)
kable(result_frame)
```

```{r, fig.width=7,fig.height=5,fig.cap="Barplot of features", fig.pos="H"}
barplot(result_frame$no_features, result_frame$alpha, xlab = 'alpha',
        ylab = 'Number of features', 
        names.arg = result_frame$alpha, col = 'royalblue')
```

\newpage

## Accuracy calculation

```{r}

## Preprocesing of text

test <- test[, index := 1:.N]

it_test = itoken(test$question_text,
             preprocessor = prep_fun,
             tokenizer = word_tokenizer,
             ids = test$index,
             progressbar = FALSE)

vocab = create_vocabulary(it_test, stopwords = stopwords(source = 'smart'))
vectorizer = vocab_vectorizer(vocab)
dtm_test  = create_dtm(it_test, vectorizer, type = 'dgCMatrix')

## Tf-idf

tfidf = TfIdf$new()
dtm_test_tfidf = fit_transform(dtm_test, tfidf)

## Adding features from the train frame

features <- colnames(dtm_train_tfidf)
missing_ft <- setdiff(features, colnames(dtm_test_tfidf))
mm <- sparseMatrix(i = nrow(dtm_test_tfidf), j = length(missing_ft))
colnames(mm) <- missing_ft
dtm_test_tfidf <- cbind(dtm_test_tfidf, mm)
dtm_test_tfidf <- dtm_test_tfidf[,features]

## Creating an object to store the results

acc_scores <- vector('list', length(alpha_range))
names(acc_scores) <- alpha_range

## Iterating

for(alpha in alpha_range){
  result <- data.table(target = test$target, 
                       fit = predict(result_model[[as.character(alpha)]],
                                     dtm_test_tfidf, type = 'class'))
  
  acc_scores[[as.character(alpha)]] <- sum(result$target == result$fit.s0)/nrow(result)
}

## Outputing scores

acc_scores <- acc_scores %>% 
  unlist() %>% 
  round(3)
result_frame <- data.table(alpha = alpha_range, accuracy = acc_scores)
kable(result_frame)
```

\newpage

# References

```{r references, results="asis", echo=F, warning=F}
PrintBibliography(bib)
```



